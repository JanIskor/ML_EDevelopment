import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    accuracy_score, confusion_matrix, roc_curve, auc, classification_report
)

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier

# –ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
@st.cache_data
def load_data():
    df = pd.read_csv("Nike US Sales.csv", sep=";")
    df['invoice_date'] = pd.to_datetime(df['invoice_date'], format='%d.%m.%Y')
    df['year'] = df['invoice_date'].dt.year
    df['month'] = df['invoice_date'].dt.month
    df['day'] = df['invoice_date'].dt.day
    df = pd.get_dummies(df, columns=[
        'retailer', 'region', 'state', 'city', 'product', 'sales_method'
    ])
    df['target'] = (df['operating_profit'] > 1000).astype(int)
    X = df.drop(['sales_id', 'invoice_date', 'operating_profit', 'target'], axis=1)
    y = df['target']
    return train_test_split(X, y, test_size=0.2, random_state=42), X.columns

# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å
st.set_page_config("–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è Nike US Sales", layout="wide")
st.title("üéì –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")
# üìä –ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
st.header("üìÇ –ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ Nike US Sales")

df = pd.read_csv("Nike US Sales.csv", sep=";")
df['invoice_date'] = pd.to_datetime(df['invoice_date'], format='%d.%m.%Y')
df['year'] = df['invoice_date'].dt.year
df['month'] = df['invoice_date'].dt.month
df['day'] = df['invoice_date'].dt.day
df['target'] = (df['operating_profit'] > 1000).astype(int)

with st.expander("üßæ –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"):
    st.write(df.describe())
    st.write("–†–∞–∑–º–µ—Ä:", df.shape)
    st.write("–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:")
    st.write(df.isnull().sum())

with st.expander("üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"):
    numeric_cols = ['price_per_unit', 'units_sold', 'total_sales', 'operating_profit']
    fig, axs = plt.subplots(2, 2, figsize=(12, 6))
    for i, col in enumerate(numeric_cols):
        sns.histplot(df[col], ax=axs[i // 2][i % 2], kde=True, color='teal')
        axs[i // 2][i % 2].set_title(col)
    st.pyplot(fig)

with st.expander("üìä –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏"):
    cat_col = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫", ['retailer', 'region', 'state', 'product', 'sales_method'])
    fig_cat, ax_cat = plt.subplots()
    sns.countplot(data=df, x=cat_col, order=df[cat_col].value_counts().index, palette="viridis", ax=ax_cat)
    plt.xticks(rotation=45)
    st.pyplot(fig_cat)

with st.expander("üß† –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π (–ø—Ä–∏–±—ã–ª—å > 1000)"):
    fig_target, ax_target = plt.subplots()
    sns.countplot(x='target', data=df, ax=ax_target, palette="Set2")
    ax_target.set_xticklabels(["0: –ù–∏–∑–∫–∞—è –ø—Ä–∏–±—ã–ª—å", "1: –í—ã—Å–æ–∫–∞—è –ø—Ä–∏–±—ã–ª—å"])
    st.pyplot(fig_target)

with st.expander("üîó –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤"):
    corr = df[['price_per_unit', 'units_sold', 'total_sales', 'operating_profit']].corr()
    fig_corr, ax_corr = plt.subplots()
    sns.heatmap(corr, annot=True, cmap="coolwarm", ax=ax_corr)
    st.pyplot(fig_corr)

(X_train, X_test, y_train, y_test), feature_names = load_data()

# –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
model_type = st.sidebar.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å", [
    "Logistic Regression", "K-Nearest Neighbors", "Decision Tree", "Random Forest", "Gradient Boosting"
])

# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
if model_type == "Logistic Regression":
    C = st.sidebar.slider("–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (C)", 0.01, 10.0, 1.0)
    model = LogisticRegression(C=C, max_iter=1000)
elif model_type == "K-Nearest Neighbors":
    n_neighbors = st.sidebar.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Å–µ–¥–µ–π", 1, 20, 5)
    model = KNeighborsClassifier(n_neighbors=n_neighbors)
elif model_type == "Decision Tree":
    max_depth = st.sidebar.slider("–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞", 1, 20, 5)
    model = DecisionTreeClassifier(max_depth=max_depth)
elif model_type == "Random Forest":
    n_estimators = st.sidebar.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤", 10, 200, 100)
    model = RandomForestClassifier(n_estimators=n_estimators)
elif model_type == "Gradient Boosting":
    n_estimators = st.sidebar.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–µ—Ä–µ–≤—å–µ–≤", 10, 200, 100)
    learning_rate = st.sidebar.slider("–°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è", 0.01, 1.0, 0.1)
    model = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate)

# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# –û–±—É—á–µ–Ω–∏–µ
model.fit(X_train_scaled, y_train)
y_pred = model.predict(X_test_scaled)
y_proba = model.predict_proba(X_test_scaled)[:, 1] if hasattr(model, "predict_proba") else None

# –ú–µ—Ç—Ä–∏–∫–∏
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred, output_dict=True)

# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ
st.subheader("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–∏")
col1, col2 = st.columns(2)

with col1:
    st.metric("–¢–æ—á–Ω–æ—Å—Ç—å", f"{accuracy:.2%}")
    st.text("–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –æ—Ç—á—ë—Ç:")
    st.dataframe(pd.DataFrame(report).transpose())

with col2:
    st.text("–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫:")
    cm = confusion_matrix(y_test, y_pred)
    fig, ax = plt.subplots()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=ax)
    ax.set_xlabel("–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ")
    ax.set_ylabel("–ò—Å—Ç–∏–Ω–Ω–æ–µ")
    st.pyplot(fig)

# ROC-–∫—Ä–∏–≤–∞—è
if y_proba is not None:
    st.subheader("üìà ROC-–∫—Ä–∏–≤–∞—è")
    fpr, tpr, _ = roc_curve(y_test, y_proba)
    roc_auc = auc(fpr, tpr)
    fig_roc, ax_roc = plt.subplots()
    ax_roc.plot(fpr, tpr, color='blue', label=f'AUC = {roc_auc:.2f}')
    ax_roc.plot([0, 1], [0, 1], 'k--')
    ax_roc.set_xlabel("False Positive Rate")
    ax_roc.set_ylabel("True Positive Rate")
    ax_roc.legend()
    st.pyplot(fig_roc)

# –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
if hasattr(model, "feature_importances_"):
    st.subheader("üîé –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    importances = model.feature_importances_
    imp_df = pd.DataFrame({"feature": feature_names, "importance": importances})
    imp_df = imp_df.sort_values("importance", ascending=False).head(10)

    fig_imp, ax_imp = plt.subplots()
    sns.barplot(x="importance", y="feature", data=imp_df, ax=ax_imp, palette="viridis")
    ax_imp.set_title("–¢–æ–ø-10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
    st.pyplot(fig_imp)

# –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
st.subheader("üîç –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π")
preview_df = pd.DataFrame({
    "–ò—Å—Ç–∏–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ": y_test.values[:10],
    "–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ": y_pred[:10]
})
st.table(preview_df)
